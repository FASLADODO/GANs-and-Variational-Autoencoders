{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN<hr>\n",
    "- This lecture: we go more in-depth on a specific GAN architecture\n",
    "- All we know is we should have 2 neural networks, but we don't know what they should look like\n",
    "- We know that we'll be working with images, for which feedforward ANNs and convolutional neural networks are appropriate\n",
    "\n",
    "\n",
    "- One very successful type of GAN was created in 2015, called DCGAN,most GANs today are based on this\n",
    "- Notable for producing high-quality, high-resolution images in a single pass\n",
    "- Before DCGANs, LAPGANs were able to generate high-res images too, but using a more complex process\n",
    "- Original paper: Radford,A.,Metz,L.,and Chintala,S.(2015).Unsupervised representation learning with deep convolutional generative adversarial networks.arXiv preprint arXiv:1511.06434\n",
    "\n",
    "\n",
    "- Stands for \"Deep Convolutional\" GAN\n",
    "- A little misleading since GANs that came before it were also deep and also contained convolution\n",
    "- Other specific characteristics differentiate DCGAN from other architectures\n",
    "- Let's now enumerate these features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "- DCGAN contain batch normalization\n",
    "![batchnomal](../images/batchnomal.PNG)\n",
    "\n",
    "## All-Convolutional Network\n",
    "- Simplifies traditional LeNet:<br>\n",
    "Instead of: conv -> pool -> conv -> pool -> ... -> linear -> ... -> out<br>\n",
    "We use: conv -> conv -> conv -> out\n",
    "- We simply use a stride > 1 to shrink the image at each layer, instead of pooling - means less things to multiply\n",
    "![conv_pooling](../images/conv_pooling.PNG)\n",
    "\n",
    "- Ex. if we want output to be 1/2 length of input:<br>\n",
    "Old way: do full conv, downsample by 2<br>\n",
    "New way: do full conv with stride = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimizer\n",
    "- An adaptive gradient descent algorithm like AdaGrad & RMSprop\n",
    "- 3 variables of interest:\n",
    "![adamoptimizer](../images/adamoptimizer.PNG)\n",
    "\n",
    "- Time varying learning rate\n",
    "- Needed because m(t) and v(t) are initialized to 0, so they are biased toward 0, this adjusts for that \n",
    "![adamoptimizer2](../images/adamoptimizer2.PNG)\n",
    "\n",
    "- Now we have all the ingredients we need to the parameter update\n",
    "- Sometimes you'll see epsilon inside the square root (doesn't really matter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky ReLU\n",
    "- Has been recommended for discriminator, with normal ReLU for generator\n",
    "- Solves problem of \"dead neurons\"\n",
    "- Whem output of node is 0, gradient is 0, is no change to parameter\n",
    "![leakyrelu](../images/leakyrelu.PNG)\n",
    "\n",
    "![dcgan](../images/dcgan.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
