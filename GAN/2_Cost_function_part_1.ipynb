{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function (part 1)<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Opposite\" Cost Functions\n",
    "- The generator and discriminator are trying to optimize the **\"opposite\"** thing\n",
    "- Let's start with the discrimination, since classification (supervised learning) is conceptually easier\n",
    "- What does the discriminator have to do ?\n",
    "- Classify an image ad *\"real\"* or *\"fake\"*\n",
    "- 2 different labels -> binary classification\n",
    "- We have seen this earlier in the course!\n",
    "![binary_classify](../images/binary_classify.PNG)\n",
    "\n",
    "#### Discriminator Cost (Binary Cross Entropy)\n",
    "- Let t = 1 mean *\"real\"* and t = 0 mean *\"fake\"*\n",
    "- Then \\\\(y = D(x) = p(image is real | image) \\in (0, 1)\\\\)<br> we can say y equals D of x which is a number between 0 and 1 representing the probability that the image x is real.<br>So that's a common shorhand way of representing the discriminator just say D of X and for completion.\n",
    "- If we want to explicitly mention the discriminator's params, we can say \\\\(y = D(x;\\theta_D)\\\\)\n",
    "- Note: x now represents any image (real or fake)\n",
    "\n",
    "#### Notation\n",
    "- Let's put a hat on x to show that's it's a fake image\n",
    "- Now we can get rid of the target it\n",
    "![binary_classify2](../images/binary_classify2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator Notation\n",
    "- Can we represent the output of the generator as a function?\n",
    "- Seesms reasonable to use G(z)\n",
    "- z represents a latent prior (same as VAE), z ~ p(z)\n",
    "- 2 steps to sample:<br>\n",
    "1) z ~ p(z)<br>\n",
    "2) x_hat = G(z)\n",
    "- Show parameters of G explicitly: \\\\(G(z;\\theta_G)\\\\)<hr>\n",
    "\n",
    "## Discriminator Cost\n",
    "- Now that we have some notation for discriminator (D) and generator (G), we can state the cost function differently:\n",
    "![Discriminator_cost](../images/Discriminator_cost.PNG)\n",
    "\n",
    "#### Batches of Data\n",
    "- We'll be looking at batches of data during training, so summing the individual negative log-likelihoods gives us the total cost:\n",
    "![Discriminator_cost_with_batch](../images/Discriminator_cost_with_batch.PNG)\n",
    "\n",
    "#### What does it approximate?\n",
    "- This is really an estimate of the expected value over all possible data\n",
    "![Discriminator_cost2](../images/Discriminator_cost2.PNG)<hr>\n",
    "\n",
    "## Generator Cost\n",
    "- Discriminator wants to discriminate between real and fake images, so it minimizes negative-log-likelihood\n",
    "- Generator wants to fool the discriminator\n",
    "- Perhaps we can just try to maximize the discriminator's cost !\n",
    "![Generator_cost](../images/Generator_cost.PNG)\n",
    "\n",
    "#### Zero-Sum Game\n",
    "- In game theory this is called a \"zero-sum-game\" because sum of all players' costs is always 0\n",
    "- You don't need to know game theory to understand this course, but it's nice to make this connection if you already know game theory\n",
    "\n",
    "#### Minmax\n",
    "- Zero-sum games are also called minimax games because solution involves a min and max\n",
    "![Minmax](../images/Minmax.PNG)\n",
    "- If you're interested you can check out Goodfellow 2014 (Generative Adversarial Nets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "## Next Step: We have a cost -> Minimize it ! \n",
    "- Once we have a cost function, it is easy to optimize it using any of the available optimizers in Tensorflow\n",
    "- Interesting situaltion we haven't seen before: 2 different neural networks, 2 different costs, in the same script, so we need 2 different optimizers\n",
    "\n",
    "#### Pseudocode\n",
    "While not converged:<br>\n",
    "&nbsp;&nbsp;&nbsp; X = get batch of real images<br>\n",
    "&nbsp;&nbsp;&nbsp; X_hat = sample batch of fake images from G<br>\n",
    "&nbsp;&nbsp;&nbsp; \\\\(\\theta_D = \\theta_D - learningrate * dj^{(D)}/d\\theta_D \\\\)<br>\n",
    "&nbsp;&nbsp;&nbsp; \\\\(\\theta_G = \\theta_G - learningrate * dj^{(G)}/d\\theta_G \\\\)<br><br>\n",
    "Note: in practice, some implementations run generator update twice for every discriminator update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
