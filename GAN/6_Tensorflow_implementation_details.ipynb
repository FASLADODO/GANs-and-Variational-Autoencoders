{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Implementation Details<hr>\n",
    "\n",
    "## Batch Normalization\n",
    "- Tensorflow has 2 functions:<br>\n",
    "tf.nn.batch_normalization(low level, built-in)<br>\n",
    "tf.conrib.layers.batch_norm\n",
    "- Generally, contrib is higher level, so less code but also less flexible\n",
    "- Built-in: need to create running mean/variance manually, update them manually\n",
    "- Contrib version creates all variables internally, but also harder to access them\n",
    "- We'll use contrib, but this will make things complicated for us in another way\n",
    "\n",
    "## Fractionally-Strided Convolution\n",
    "- It's strange. but the strangeness is logically consistent\n",
    "- In TF, convolution filter is specified like:<br>\n",
    "(filter height, filter width, # feature maps in, # feature maps out)\n",
    "- Strides are specified like:<br>\n",
    "(1, vertical stride, horizontal stride, 1)\n",
    "\n",
    "\n",
    "- We specify the shapes and strides as if the \"input to the function\" were actually the \"output of a regular convolution\"\n",
    "- If you think of fractionally-strided convolution as a \"forward\" operation, then you might use:<br>\n",
    "(filter height, filter width, # feature maps in, # feature maps out)\n",
    "- But a forward convolution that would have generated the function input as its output would actually have had the filter shape:<br>\n",
    "(filter height, filter width, # feature maps out, # feature maps in)\n",
    "\n",
    "\n",
    "- Same theme applies to strides too\n",
    "- You'd think because it's \"fractionally strided\", we could use stride = 0.5 -> Error!\n",
    "- If we \"pretend\" out output generates the input via a convolution, then THAT stride would be 2\n",
    "- For a regular forward convolution, output would be 1/2 size of input\n",
    "- Since this is a \"backward\" convolution, output is twice the size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conv2d_transpose arguments\n",
    "- For regular conv, we use padding = \"SAME\" -> yields convenient shape calculations\n",
    "- Conv transpose doesn't have this, instead we have an argument for output_shape\n",
    "- \"output_shape\" refers to actual output of conv transpose function, not the \"output as if we were doing forward conv\"\n",
    "- If you specify output_shape incorrectly -> error!\n",
    "- Makes you wonder why it is needed at all\n",
    "\n",
    "## output_shape example\n",
    "- Input to function:(N, 8, 8, 64)\n",
    "- Stride = 2\n",
    "- Filter size: (5, 5, 32, 64)\n",
    "- Vocabulary:<br>\n",
    "input = input to conv2d_transpose<br>\n",
    "output = output from conv2d_transpose\n",
    "- Is filter the right shape? Last dim(64) should match # feature maps of \"input\"<br>\n",
    "Since \"our input\" == \"output of a forward conv\"\n",
    "\n",
    "\n",
    "- Input to function: (N, 8, 8, 64)\n",
    "- Stride = 2\n",
    "- Filter size: (5, 5, 32, 64)\n",
    "![output_shape_ex](../images/output_shape_ex.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Design\n",
    "- I like to use classes for each layer\n",
    "- One function to create params, one function to use them\n",
    "\n",
    "\n",
    "```python\n",
    "class MyLayer:\n",
    "    def __init__(m_in, m_out):\n",
    "        W = random(m_in, m_out)\n",
    "        b = random(m_out)\n",
    "    \n",
    "    def forward(X):\n",
    "        return f(XW + b)\n",
    "\n",
    "```\n",
    "\n",
    "- Layers in Tensorflow's contrib module do both of these at the same time\n",
    "- On one hand, it's easier because you can do both in one line:\n",
    "\n",
    "```python\n",
    "output = contrib_layer(input) # create weights automatically\n",
    "```\n",
    "\n",
    "- It's ok for params to be internal, b/c in TF we don't need direct access optimizer will update them automatically\n",
    "\n",
    "\n",
    "Actually, that's not true in our case ! We need 2 optimizers: 1for G , 1 for D We update only one set of params at a time<br>\n",
    "So we need another special function to collect the params to tell TF what to optimize\n",
    "\n",
    "## What's the problem?\n",
    "- Remember how a GAN works\n",
    "- We have 2 batches of data to pass to discriminator (hence, need to call forward() twice)\n",
    "- But if we call our batch_norm(input) twice, it's going to create its internal params all over again !\n",
    "![Discriminator.PNG](../images/Discriminator.PNG)\n",
    "\n",
    "## What's the solution\n",
    "The solution is that the batch norm layer accepts an argument called **reuse** you pass in true if you want to reduce the weights of this layer\n",
    "\n",
    "```python\n",
    "batch_norm(\n",
    "    inputs,\n",
    "    reuse = None,\n",
    ")\n",
    "```\n",
    "\n",
    "- Unfortunately, this introduces other complexities too\n",
    "- It's just a function call - how does TF find the previously created weights?\n",
    "<br>(Remember, i've called this same function multiple times already)\n",
    "- We need scopes (reminds me of namespaces in C++), and a name for each layer\n",
    "- Makes the code messier, but benefits outweight costs\n",
    "\n",
    "## Dont' forget! Batch norm train vs test\n",
    "\n",
    "```python\n",
    "batch_norm(\n",
    "    inputs,\n",
    "    reuse = None,\n",
    "    is_training = True,\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "- Now we need yet another flag to the layer, is_training\n",
    "- Instead of just a clean layer.forward(X), we now have layer.forward(X, reuse, is_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Implementation Summary\n",
    "- Non-trivial obstacles in getting a DCGAN working in TF\n",
    "- Convolutional arithmetic, FS-conv treats input as output and output as input in a forward conv\n",
    "- Batch norm, reuse, scope, name so TF can track down your variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
