{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization Review<hr>\n",
    "- Recall that before we input data into many ML algorithms, we like to normalize the data first\n",
    "- \\\\(z = (x - \\mu)/\\sigma\\\\)<br>\n",
    "Recall that normalization means subtracting the mean and them dividing by the standard deviation. in other words we make sure that the data has a mean of 0 and a variance of 1.\n",
    "- With batch norm, instead of manually normalizing data first, we do a normalization at every layer of the neural net\n",
    "- Building in normalization into the neural net, vs doing the calculation yourself before it is input into the neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it work?\n",
    "- It's called *\"batch normalization\"* because we'll be doing batch gradient descent\n",
    "- During training, we consider a small batch of data for each gradient descent step<br>\n",
    "\\\\(X_\\beta =\\\\) next batch of data<br>\n",
    "\\\\(\\mu_\\beta = mean(X_\\beta)\\\\)<br>\n",
    "\\\\(\\sigma_\\beta = std(X_\\beta)\\\\)<br>\n",
    "\\\\(Y_\\beta = (X_\\beta - \\mu_\\beta)/\\sigma_\\beta\\\\)\n",
    "\n",
    "- Only applies during training, since only then will we have batches (we'll do something else for testing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Issue\n",
    "- Important: When does batch normalization actually happen ?\n",
    "![batchnomal2](../images/batchnomal2.PNG)\n",
    "\n",
    "## Pretty Simple, No?\n",
    "\\\\(X_\\beta = \\\\) next batch of data (small note: \"X\" refers to the activation here)<br>\n",
    "\\\\(\\mu_\\beta = mean(X_\\beta)\\\\)<br>\n",
    "\\\\({\\sigma_\\beta}^2 = var(X_\\beta)\\\\)<br>\n",
    "\n",
    "- One more small detail: add a small number to denominator to avoid dividing by 0\n",
    "\n",
    "\\\\(Y_\\beta = (X_\\beta - \\mu_\\beta)/sqrt({\\sigma_\\beta}^2+\\epsilon\\\\))\n",
    "\n",
    "But we are misiing one step!\n",
    "\n",
    "## Counter-Intuitive Step\n",
    "- After going through all that trouble to normalize the data, we change its scale and location to something else !\n",
    "![counter_intuitive](../images/counter_intuitive_step.PNG)\n",
    "\n",
    "- Why do we \"un-standardize\" out data after standardizing it?\n",
    "- Standardization may not be good (we don't know)\n",
    "- Let gradient descent figure out what's best by updating \\\\(\\gamma, \\beta\\\\)\n",
    "\n",
    "\n",
    "- Suppose standardization is good -> then the neural network will learn that \\\\(\\gamma = 1, \\beta = 0\\\\)\n",
    "- \\\\(\\gamma, \\beta\\\\) should be whatever minimizes our cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We still have a problem\n",
    "- We know how to train, but not how to test\n",
    "- Suppose we want to make a prediction for 1 data point - if we subtract its mean(which is just itself), we get the vector 0!\n",
    "- Would be nice if we kept track of a **\"global mean\"** and **\"global variance\"** during training, and subtract those from the test samples\n",
    "- That's exactly what we do! (looks like like RL / RMSprop / Adam smoothing)\n",
    "<br>\n",
    "for each batch B:<br>\n",
    "\\\\('_\\mu = decay*'_\\mu + (1 - decay) * {'_\\mu}_{\\beta}\\\\)<br>\n",
    "\\\\(\\sigma^2 = decay*\\sigma^2 + (1 - decay) * \\sigma^2_\\beta\\\\)<br>\n",
    "\n",
    "- Theoretically, could just use sample mean/var of all training data, but may not scale\n",
    "\n",
    "### Batch norm test mode\n",
    "\\\\(\\mu, \\sigma^2\\\\) collected during training<br>\n",
    "\\\\(x_{test} = (x_{test} -\\mu)/\\log(\\sigma^2 + \\epsilon)\\\\)<br>\n",
    "\\\\(y_{test} = \\gamma x_{test}+\\beta\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "- You could try to implement these from scratch (I think it would be a great exercise)\n",
    "- But instead, use these:\n",
    "- TF: tf.nn.batch_normalization or tf.contrib.layers.batch_norm\n",
    "- Theano:<br>\n",
    "from theano.tensor.nnet.bn import batch_normalization_train, batch_normalization_test\n",
    "![batchnomal_implementation](../images/batchnomal_implementation.PNG)\n",
    "\n",
    "## One Last Note\n",
    "- These are all element-wise operations\n",
    "- It applies equally to scalars, vectors, images\n",
    "- Difference for images (e.g. convolution output): we think of # feature maps as the \"output\"<br>\n",
    "So for image of (H,W,C) = (28,28,512),\\\\(\\gamma,\\beta\\\\) are size 512 NOT 28 x 28x 512!\n",
    "- Direct analogy to a fully connected layer, \\\\(\\gamma,\\beta\\\\) are same size as # hidden usnits(a.k.a. # of features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
