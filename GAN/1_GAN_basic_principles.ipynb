{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Basic Principles<hr>\n",
    "## Generative Adversarial Networks (GANs)\n",
    "- New section starting now: GANs\n",
    "- Considered one of the most interesting developments in 2016 (although they were invented in 2014)\n",
    "- Main attraction: GANs are extremely good at generating realistic, believable samples\n",
    "- Have been aplplied to very interesting applications, e.g. turning sketches into photo-realistic images\n",
    "\n",
    "## Basic Principles\n",
    "- High-level discussion\n",
    "- What has every model we've discussed have in common? (Bayes Classifier, Variational Autoencoder)\n",
    "- We worked closely with probability distributions\n",
    "- With GANs, we won't be dealing with explicit distributions\n",
    "- Goal when training a GAN is to reach Nash equilibrium of a game\n",
    "\n",
    "## Comparison with other probabilistic models \n",
    "- Previous Unsupervised Deep Learning Course: Restricted Boltzmann Machines\n",
    "- Can generate samples using Monte Carlo sampling (requiring 1000s of iterations)\n",
    "- One disadvantage: no way to tell how many iterations is enough\n",
    "- GANs can generate samples in a single pass \n",
    "\n",
    "## Objective Quality\n",
    "- Usually in ML, we have some loss function like the negative log-likelihood that we want to optimize\n",
    "- We can tell how good a model is by looking at its loss\n",
    "- In supervised learning, this is very explicit: we compare \\\\(R^2\\\\) or accuracy<br> E.g. 80% is better than 60%<br>These things are objective\n",
    "- Assessing sample quality is very subjective (\"does it look real\")<br>No numerical measure for this\n",
    "- We know GANs produce better samples than VAEs, but no number tell us this\n",
    "- We just have to use our senses!\n",
    "\n",
    "## How do GANs work?\n",
    "![gans_work](../images/gans_work.PNG)\n",
    "\n",
    "## GAN Analogy\n",
    "![GAN_analogy](../images/GAN_analogy.PNG)\n",
    "\n",
    "## Is it really that simple?\n",
    "- GAN doesn't contain any components we don't already know about\n",
    "- We know how to build neural networks, now just build 2 of them\n",
    "- We know how to build objective functions, now just make one the opposite of the other<br>\n",
    "And let Tensorflow/Theano do gradient descent with autodiff\n",
    "- You could plausibly start creating a prototype right now!\n",
    "- But we will take advantage of hard work already done by deep learning resarchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
