{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder Architecture<hr>\n",
    "\n",
    "## Variational Autoencoder Architecture\n",
    "![variational_architecture](../images/variational_architecture.PNG)\n",
    "\n",
    "As you know the variational auto encoder is a neural network. \n",
    "\n",
    "It comes in two parts the encoder and decoder. \n",
    "\n",
    "They're split in the middle which as we discussed is typically smaller than the input size. \n",
    "\n",
    "We typically call the values at this particular hidden layer Z and they represent the latent variable representation of the input data.\n",
    "\n",
    "## How does the data flow?\n",
    "- KEY POINT: does not work like a traditional autoencoder\n",
    "- Traditional autoencoder (wih one hidden unit):\n",
    "![traditional_autoencoder](../images/traditional_autoencoder.PNG)\n",
    "\n",
    "## Encoder\n",
    "![encoder](../images/encoder.PNG)\n",
    "Something strange happends at the end of the encoder. in particular we don't get a value but we get a distribution or more precisely we get the parameters\n",
    "\n",
    "**In particular, the encoder outputs a mean and variance, which represent a Gaussian**\n",
    "\n",
    "Remember Bayes and machine learning is all about learning distributions not learning point estimates. so instead of actually finding *z* we are finding *q of z* a distribution that tell us that pdf z will get in to specifics of just how to calculate those parameters later but for now let's just say that somehow we can turn the output of the encoder into a mean and variance and then q of z is just a Gaussian with that mean and variance\n",
    "\n",
    "## Decoder \n",
    "![decoder](../images/decoder.PNG)\n",
    "So, that's the first half of the variational auto encoder we get to the end of the encoder. now we have to go through the decoder so we have a distribution *q of z*\n",
    "\n",
    "From this we need actual numbers to pass in the rest of the neural network. so what we do is try sample from q of z.\n",
    "\n",
    "Now that we have a sample *z vector* we can pass it through that decoder as usual.multiply by the way add the bias apply the activation function and so on\n",
    "\n",
    "Finally we get to the output of the decoder. what do we do here, not surprisingly, this also a distribution, Let's assume our input is a binary variable so that our output is also a binary variable. in other words they only take values 0 and 1 and this is **Bernoulli distribution** and that is represented by one parameter which tells us the probability of getting a 1.\n",
    "\n",
    "Sigmoid it gives us a value in between 0 and 1 and therefore a sigmoid is the appropriate activation function, so that output of the decoder can represent Bernoulli distributions.\n",
    "\n",
    "- Because the output is a distribution, it's going to affect how we use it\n",
    "- with binary classification \\\\(p(y = 1 | x)\\\\), we just round to get prediction\n",
    "- With variational autoencoders, paradigm is different\n",
    "- From a probability distribution, we can generate samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- How to do the forward pass in a variational autoencoder\n",
    "- Encoder + Decoder\n",
    "- End of encoder is a bottleneck \n",
    "- z = compact code / compressed version of input\n",
    "- Encoder really gives us q(z), a Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
