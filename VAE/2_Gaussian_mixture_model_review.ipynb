{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model Review\n",
    "### Extending bayes classifier<hr>\n",
    "- Single Gaussian model learns blurry images - why?\n",
    "- We tried to force a single Gaussian to fit a multi-modal distribution\n",
    "- Recall: mode is local max in pdf\n",
    "- Makes sense for images of digits\n",
    "- Not everyone will write digits the same way\n",
    "- But there should be a finite number of clusters\n",
    "\n",
    "## Multi-modal distributions\n",
    "![multi_modal_distributions](../images/multi_modal_distributions.PNG)\n",
    "- Suppose we collect 1000 images of written 2s\n",
    "- 500 similar to the left, 500 similar to the right\n",
    "- Then we'd call this a bi-modal distribution\n",
    "- In reality, there will be more, but still finite\n",
    "- There are only so many ways to write a 2 until it ceases to look like a 2\n",
    "\n",
    "## How can we model a multi-modal distribution?\n",
    "- We'll use a pre-built GMM (Sci-Kit Learn)\n",
    "- What's important: this can fit multiple Gaussians in different proportions to approximate a multi-modal dist\n",
    "\n",
    "## Interesting Facts about GMMs\n",
    "- GMM is a latent variable model, and unsupervised learning is all about latent variables!\n",
    "- We call the latent variable **'Z'** - it represents **\"which cluster x belongs to \"**\n",
    "- The marginal of x looks a lot like our Bayes classifier\n",
    "- 2 clusters: \\\\(p(x) = p(z=1)p(x|z=1) + p(z=2)p(x|z=2)\\\\)\n",
    "- \\\\(p(z)\\\\): prior probability that any x belongs to a cluster\n",
    "- \\\\(p(z)\\\\): categorical / discrete distribution\n",
    "- \\\\(p(x|z)\\\\): Gaussian\n",
    "\n",
    "### The prior\n",
    "- \\\\(p(z)\\\\) tells us, without looking at any x, which cluster you're likely to belong to \n",
    "- Ex. ask 1000 people if they have a disease (yes = 1, no = 0)\n",
    "- \\\\(p(z=1)\\\\) = # people who said yes / 1000\n",
    "\n",
    "### Common Mistake\n",
    "- \\\\(p(z)\\\\) is not the same as \\\\(p(z|x)\\\\):\n",
    "- Analogy:\n",
    "- \\\\(p(z)\\\\) - frequency of disease in population\n",
    "- \\\\(p(z|x)\\\\) - patient goes to doctor's office and performs a test\n",
    "- x is the test data (clearly, having this data would alter the probability of whether or not you have the disease)\n",
    "\n",
    "## Assigning a cluster\n",
    "- Given an x, how can we find which cluster z it belongs to?\n",
    "- Use Bayes rule!\n",
    "- \\\\(p(z|x) = p(x|z)p(z)/p(x)\\\\)\n",
    "- Where \\\\(p(x)\\\\) is called the *\"evidence\"*, and is just the sum of \\\\(p(z)p(x|z)\\\\) over all z\n",
    "\n",
    "## More interesting facts about GMMs\n",
    "- GMM is trained using expectation-maximization(EM)\n",
    "- Details not important, but **\"Where it fits\"** is interesting\n",
    "- We use EM for latent variable models because we can't find a closed-form maximum-likelihood solution\n",
    "- EM is iterative (likelihood improves at each step)\n",
    "- But still, no special objective, we are still just doing maximum-likelihood\n",
    "- Simple example: finding the mean height of students in your class assuming a Gaussian distribution\n",
    "- We find sample mean/stddev by maximizing likelihood L wrt params, i.e. dL/d\\\\(\\theta\\\\)=0\n",
    "- EM: cannot solve dL/d\\\\(\\theta\\\\)=0\n",
    "\n",
    "## Why is it important?\n",
    "- Variational inference is a key component of variational autoencoders\n",
    "- Variational inference can be seen as a Bayesian extension of EM\n",
    "![variational_inference](../images/variational_inference.PNG)\n",
    "\n",
    "## An important application of variational inference\n",
    "- Recall: one weakness of K-Means Clustering / GMMs is we have to choose K, the number of clusters\n",
    "- If you choose wrong, model is bad\n",
    "- The variational inference version of GMM contains an infinite number of clusters\n",
    "- Most remain empty, and so VI-GMM automatically finds the number of clusters for you \n",
    "- We will assume it can do so effectively\n",
    "- We will use Sci-Kit Learn's built-in VI-GMM\n",
    "\n",
    "## More graphical modeling\n",
    "![More_graphical_modeling](../images/More_graphical_modeling.PNG)\n",
    "\n",
    "## Summary\n",
    "- Mostly new concepts and ideas\n",
    "- Implementation should be very simple\n",
    "- Just replace Gaussian (mean/cov) with Variational GMM:\n",
    " - from sklearn.mixture import BayesianGaussianMixture\n",
    "- Already comes with fit() and sample() functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
