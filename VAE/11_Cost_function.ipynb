{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function<hr>\n",
    "## Training an autoencoder\n",
    "- Machine learning models typically have 2 functions we're interested in: leaning and inference\n",
    "- **\"Inference\"** - too broad and overused, but generalizes the concept of the **\"forward\"** direction\n",
    "- Supervised learning: prediction\n",
    "- Unsupervised learning: transforming data into latent representation\n",
    "- Sci-Kit Learn: fit(X,Y)/predict(X) or fit(X)/transform(X)\n",
    "- Next step: how to fit / train / learn?\n",
    "- As usual, we start with a cost function, then try to minimize it\n",
    "\n",
    "## Cost Function\n",
    "- A little tricky\n",
    "- Will look a little strange compared to what we're used to (common theme in this course)\n",
    "- Would be beneficial to know about variational inference \n",
    "- But not required - we will derive a solution from first principles\n",
    "\n",
    "\n",
    "- Outline:\n",
    "- Start with what it is\n",
    "- Then ask: \"Does it make sense?\"\n",
    "- i.e. does it decrease as the autoencoder improves at its task?\n",
    "- Then, a theoretical perspective considering deep learning / machine learning cost functions in general \n",
    "- Later: look at the cost function from a probabilistic perspective\n",
    "- Since variational inference is part of Bayesian ML, the probabilistic perspective is of particular interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ELBO \n",
    "- We call the objective function the **\"ELBO\"** - evidence lower bound \n",
    "- Reason why will be explained later\n",
    "- We want to maximize the ELBO, therefore our **\"cost\"** (which is something to minimize) is - ELBO\n",
    "\n",
    "![elbo](../images/elbo.PNG)\n",
    "## Expected Log-Likelihood\n",
    "- Looks strange, but it's just a very fancy way of doing what we usually do\n",
    "- What is the expected value of a log probability?\n",
    "- (Negative) cross-entropy !\n",
    "- Since x and x_hat are bernoulli probabilities:\n",
    "![log_likelihood](../images/log_likelihood.PNG)\n",
    "\n",
    "\n",
    "- If input/output are Gaussian - cross-entropy is squared error + some constants\n",
    "- Same losses we used in our regular autoencoder\n",
    "![log_likelihood2](../images/log_likelihood2.PNG)\n",
    "\n",
    "## KL-Divergence\n",
    "- KL divergence between \\\\(q(z|x)\\\\) and \\\\(p(z)\\\\)\n",
    "- In Bayesian ML we call \\\\(p(z)\\\\) the \"prior\"\n",
    "- It is up to is to \"choose it\"\n",
    "- For convenience, we choose \\\\(p(z) = N(0,1)\\\\)\n",
    "- No theoretical underpinning, it's just convenient\n",
    "- Weakness of Bayesian ML: poorly chosen prior can lead to bad results in good model\n",
    "![KL_divergence](../images/KL_divergence.PNG)\n",
    "![KL_divergence2](../images/KL_divergence2.PNG)\n",
    "\n",
    "- What does KL-Divergence do?\n",
    "- Allows us to compare 2 probability distributions\n",
    "- If q = p, KLD = 0\n",
    "- If q != p, KLD > 0\n",
    "- So this term encourages \\\\(q(z|x)\\\\) to \"be like\" \\\\(p(z) = N(0,1)\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does it make sense?\n",
    "- Cost function is made up of 2 parts\n",
    "- First part: Tells us how close our output is to target (we always have this, whether we are doing classification, regression, or reconstructing input)\n",
    "- Second part: Regularization\n",
    "- Common construction in machine learning, including deep learning models and SVMs(Support Vector Machines)\n",
    "- Make sense: Cost = Target-Output Penalty + Regularization Penalty\n",
    "\n",
    "## Good Enough for Now\n",
    "- We don't know \"why\" the cost function is this way, but we know it make sense\n",
    "- We will look at why it takes this form later (with much more theory and math)\n",
    "- Not needed for implementation (coming next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
