{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Reparameterization Trick<hr>\n",
    "- Look at one component of VAE in more detail\n",
    "- Tensorflow conveniently hides this detail from us, but unfortunately it just looks like magic\n",
    "- In more detail: how do we sample z ~ \\\\(q(z|x)\\\\)\n",
    "- Not entirely clear how the weights of the encoder, \\\\(q(z|x)\\\\), can be updated<br> i.e. what is the gradients of a sampleing operation?\n",
    "\n",
    "\n",
    "- We have kind of seen this before\n",
    "- Think way back to \"The Numpy Stack in Python\"\n",
    "- One exercise was to sample from an arbitrary Gaussian using just Numpy\n",
    "- We know that np.random.randn sample from N(0,1)\n",
    "- How can we sample from \\\\(N(\\mu, \\sigma^2)\\\\) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "e = np.random.randn()\n",
    "\n",
    "x =  \\\\(\\mu+\\sigma e\\\\)\n",
    "\n",
    "\n",
    "Then, X ~ \\\\(N(\\mu, \\sigma^2)\\\\)\n",
    "\n",
    "If you are not convinced this works, print out the sample mean / stddev in Numpy\n",
    "\n",
    "## Why does it work?\n",
    "- Think of the opposite operation\n",
    "- We like to standardize / normalize data before using a ML algorithm on it\n",
    "##### \\\\(Z_i = \\frac{x_i - \\mu}{\\sigma} \\leftarrow\\rightarrow x_i = Z_i\\sigma+\\mu\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is this relevant?\n",
    "- Seems very basic so far \n",
    "##### Encoder outputs : \\\\(\\mu(x;\\theta), \\sigma(x;\\theta)\\\\)<br>\n",
    "Explicitly showing that mu and sigma are functions of x and theta the parameters of the encoder.<br>\n",
    "##### \\\\(q(z|x) = N(\\mu(x;\\theta), \\sigma^2(x;\\theta)),z \\\\)~ \\\\(q(z|x)\\\\)<br>\n",
    "After we find q of the given x which is a Gaussian with mean mu and standard deviation sigma we need to take a sample from this distribution.<br>\n",
    "##### \\\\(z = \\mu(x;\\theta)+\\sigma(x;\\theta)\\varepsilon, \\varepsilon\\\\)~\\\\(N(0,1)\\\\)<br>\n",
    "We'll consider what would happen if we used our Numpy method of obtaining the sample we would actually grap a sample from \\\\(N(0, 1)\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does this help us?\n",
    "- To get the output of the decoder, I pass in z\n",
    "- z is now **not stochastic** with respect to \\\\(\\theta\\\\)\n",
    "- In other words if i let my objective be J \n",
    "- Now i can easily calculate the gradient with respect to \\\\(\\theta\\\\)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
