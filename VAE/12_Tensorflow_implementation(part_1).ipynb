{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Implementation (part_1)<hr>\n",
    "## Basic Outline\n",
    "- VAE is still just a neural network that has a cost function, and we want to minimize it via gradient descent\n",
    "- At the very least:\n",
    " - Build a Variational Autoencoder class with a fit(x) function\n",
    " - Performs gradient descent on the cost we've defined\n",
    " - Plots the cost so we can confirm it decreases\n",
    "\n",
    "## Forward Direction\n",
    "- In order to build the cost, I must be able to get x_hat\n",
    "- We already know how to write the encoder: X -> \\\\(q(z|x)\\\\)\n",
    "- Next step: sample z ~ \\\\(q(z|x)\\\\)\n",
    "- Next step is decoder: z -> \\\\(p({x}{\\_reconstruction} = 1 | x\\\\)\n",
    "- The hard part is taking the sample. Why?\n",
    "- How can the parameters of the encoder be differentiable after drawing a sample?\n",
    "- In Tensorflow, we'll need some special library functions\n",
    "\n",
    "## Building the Cost\n",
    "- 2 parts: expected *log-likelihood* and *KL-divergence*\n",
    "- First, the expected log-likelihood\n",
    "\n",
    "\n",
    "1) Can use Tensorflow's binary cross entropy function<br>\n",
    "2) Create a Bernoulli distribution object for x_hat, calculate the log pdf of x\n",
    "\n",
    "- If you are not sure why these are the same, i'd encourage writing them out on paper or writing some Numpy code to prove it to yourself\n",
    "- But do not use the direct definition since it's not numerically stable\n",
    "\n",
    "\n",
    "- 2nd part: KL-divergence\n",
    "- Luckily, Tensorflow comes with a function:\n",
    "- tf.contrib.distributions.kl_divergence(dist1, dist2)\n",
    "- It is also possible to derive the KL-divergence between 2 Gaussian in closed-form\n",
    "- Tensorflow is more like an exercise in reading documentation rather than machine learning\n",
    "- Now that we have both terms of ELBO, it is straightforward to create an optimizer / do gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What functions should the VAE comopute?\n",
    "- So far:\n",
    "- We know we want x_reconstructed from x (the input)\n",
    "- Call this the posterior predictive sample\n",
    "- We also (not strictly necessary, but useful for us) want:\n",
    "- Draw a sample from p(z) = N(0, 1), then decode\n",
    "- Call this the prior predictive sample\n",
    "- Later, we will want to map each part of the latent space of z to an image\n",
    "- For this, we will need to be able to calculate output of decoder from a given z(don't sample, just output Bernoulli means)\n",
    "\n",
    "## Summary\n",
    "- Build the forward operation using stochastic tensor to sample z\n",
    "- Build the cost using built-in TF functions, optimize it\n",
    "- Add some standard operations: posterior predictive sample, prior predictive sample with z ~ N(0, 1), and prior predictive means from given z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
