{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Perspective<hr>\n",
    "- Where does our cost function come from?\n",
    "- Consider \\\\(p(z|x)\\\\)\n",
    "- We would like to know what it is\n",
    "- Our encoder approximates this ! \\\\(q(z|x)\\\\) approximates \\\\(p(z|x)\\\\)\n",
    "- But why do we care about \\\\(p(z|x)\\\\)\n",
    "- \\\\(p(z|x)\\\\) is our posterior\n",
    "- We want a good mapping from x -> z "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "- Gaussian Mixture Models and Hidden Markov Models\n",
    "- GMM:\n",
    " - \\\\(p(z|x)\\\\) tells us \"which cluster x belongs to\"\n",
    " - z is cluster identity\n",
    " - in clustering that is our intended goal - given an x, find its cluster\n",
    "- HMM:\n",
    " - z refers to \"hidden state\"\n",
    " - Ex.x = word in a sentence, z = parts-of-speech tag\n",
    " \n",
    "## Classification\n",
    "- In classification, we have targets \\\\(p(y|x)\\\\)\n",
    "- Neural network predictions are \\\\(q(y|x)\\\\)\n",
    "- Typically, \\\\(p(y = k|x) = 1\\\\) for some class k, and \\\\(p(y = k'|x) = 0\\\\) for all \\\\(k' != k\\\\)\n",
    "- The correct cost function for classfication:\n",
    " - Cross-Entropy\\\\([p(y|x), q(y|x)]\\\\)\n",
    "- Cross-Entropy = KL-Divergence + constant (gradient for both is the same)\n",
    "- Only difference:\n",
    " - Supervised learning: we are given \\\\(y / p(y|x)\\\\)\n",
    " - Unsupervised learning: we don't know \\\\(z/p(z|x)\\\\)\n",
    "- Approximating \\\\(p(z|x)\\\\) is the unsupervised equivalent of approximating \\\\(p(y|x)\\\\)\n",
    "\n",
    "## Important\n",
    "- Supervised learning: we want to find \\\\(p(y|x)\\\\), the true target\n",
    "- Unsupervised leaning: we want to find \\\\(p(z|x)\\\\), the true unobserved variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL-Divergence \n",
    "- Now that we know we want \\\\(q(z|x)\\\\) to approximate \\\\(p(z|x)\\\\), how can we formulate it as a cost function?\n",
    "- KL-divergence ! \n",
    "\n",
    "##### Before\n",
    "![kl_divergence2](../images/kl_divergence2.PNG)\n",
    "\n",
    "##### Expected value \n",
    "![expected_value](../images/expected_value.PNG)\n",
    "let's use the expected value instead\n",
    "\n",
    "##### After\n",
    "![kl_divergence3](../images/kl_divergence3.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Rule\n",
    "![bayes_rule](../images/bayes_rule.PNG)\n",
    "<br>\n",
    "## Cont...\n",
    "![bayes_rule2](../images/bayes_rule2.PNG)\n",
    "<br>\n",
    "## Why is this interesting?\n",
    "![elbo2](../images/elbo2.PNG)\n",
    "<br>\n",
    "## Left-hand side\n",
    "![elbo-left](../images/elbo_left.PNG)\n",
    "<br>\n",
    "\n",
    "## Re-arranging\n",
    "![elbo3](../images/elbo3.PNG)\n",
    "<br>\n",
    "\n",
    "## ELBO\n",
    "![elbo4](../images/elbo4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- Originally, i gave you a cost function and stated that we will minimize it\n",
    "- From a practical standpoint, it made sense\n",
    "- Cost = reconstruction error + regularization penalty\n",
    "- Call it the ELBO\n",
    "- In deep learning / machine learning in general - target-output- penaly + regularization penalty is often a starting premise\n",
    "- Next, the probabilistic perspective: accurately estimate \\\\(p(z|x)\\\\), justifying using of ELBO\n",
    "\n",
    "## Small note about variantional inference\n",
    "- For those of you who have studied VI before\n",
    "- Common technique for solving VI problems is mean-field approximation\n",
    "- Not relevant if you don't already know what it is\n",
    "- Like me, you may have been looking for something resembling the mean-field approximation\n",
    "- But that is now what is being used for the variational autoencoder ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
